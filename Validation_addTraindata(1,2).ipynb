{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[simple decision tree]\n",
      "0.738315696649 0.0\n",
      "[ExtraTrees]\n",
      "0.570987654321 0.0\n",
      "[Random Forest]\n",
      "0.645723104056 0.0\n",
      "[Gradiant boost]\n",
      "0.925264550265 0.0\n",
      "[xgboost]\n",
      "0.924382716049 0.0\n",
      "[SVC]\n",
      "0.898368606702 0.0\n"
     ]
    }
   ],
   "source": [
    "#cross validation between decision tree, extra tree, random forest, g boost, xg boost, svc\n",
    "#add training data (cover type 1,2)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import*\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "train = pd.read_csv('./train.csv').iloc[:, 1:]\n",
    "def merge(df, count):\n",
    "    if count >= 5: ###cover type 1,2를 얼마만큼 중복생성할 것인지 결정.### \n",
    "        return df\n",
    "    add_1 = train.loc[train['Cover_Type'] == 1]\n",
    "    add_2 = train.loc[train['Cover_Type'] == 2]\n",
    "    new_df = pd.concat([add_1, add_2, df], axis=0)\n",
    "    return merge(new_df, count+1)\n",
    "\n",
    "dfX = train.iloc[:, :-1]\n",
    "dfy = train.iloc[:, [-1]]\n",
    "\n",
    "\n",
    "num = 5 #교차검증 횟수\n",
    "#결과를 저장할 리스트\n",
    "re_tree = []\n",
    "re_extra = []\n",
    "re_xg = []\n",
    "re_svc = []\n",
    "re_gb = []\n",
    "re_random = []\n",
    "\n",
    "#hyper parameters for decision tree, random forest, xg boost\n",
    "n_es = 200\n",
    "m_d = 10\n",
    "\n",
    "for i in range(num):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dfX, dfy, test_size=0.3)\n",
    "    #train, test 데이터가 분리된 상태에서 train 데이터에 대해서만 covertype 1,2 추가 \n",
    "    new_train = pd.concat([X_train, y_train], axis=1)\n",
    "    new_train = merge(new_train, 0)\n",
    "    X_train = new_train.iloc[:, :-1]\n",
    "    y_train = new_train.iloc[:, [-1]]\n",
    "    simple_tree = DecisionTreeClassifier(criterion = 'entropy', max_depth=m_d).fit(X_train, y_train)\n",
    "    extra = ExtraTreesClassifier(max_depth=m_d, n_estimators=n_es).fit(X_train, y_train.values.ravel())\n",
    "    random = RandomForestClassifier(n_estimators=n_es, max_depth=m_d).fit(X_train, y_train.values.ravel())\n",
    "    xg = XGBClassifier(n_estimators=n_es, learning_rate=0.1, max_depth=m_d, objective='multi:softmax') \n",
    "    g_boost = GradientBoostingClassifier(n_estimators=200, max_depth=10).fit(X_train, y_train.values.ravel())\n",
    "    result = xg.fit(X_train, y_train.values.ravel())\n",
    "    model_svc = make_pipeline(StandardScaler(), SVC(C=1000, gamma = 0.1)).fit(X_train, y_train.values.ravel())\n",
    "    pred_y1 = simple_tree.predict(X_test)\n",
    "    pred_y2 = extra.predict(X_test)\n",
    "    pred_y3 = result.predict(X_test)\n",
    "    pred_y4 = model_svc.predict(X_test)\n",
    "    pred_y5 = random.predict(X_test)\n",
    "    pred_y6 = g_boost.predict(X_test)\n",
    "    #print(tree1.feature_importances_)\n",
    "    \n",
    "    re_tree.append(accuracy_score(y_test, pred_y1))\n",
    "    #print(classification_report(y_test, pred_y1, target_names = ['1', '2', '3', '4', '5', '6', '7']))\n",
    "    re_extra.append(accuracy_score(y_test, pred_y2))\n",
    "    #print(classification_report(y_test, pred_y2, target_names = ['1', '2', '3', '4', '5', '6', '7']))\n",
    "    re_xg.append(accuracy_score(y_test, pred_y3))\n",
    "    #print(classification_report(y_test, pred_y3, target_names = ['1', '2', '3', '4', '5', '6', '7']))\n",
    "    re_svc.append(accuracy_score(y_test, pred_y4))\n",
    "    re_random.append(accuracy_score(y_test, pred_y5))\n",
    "    re_gb.append(accuracy_score(y_test, pred_y6))\n",
    "    #print(classification_report(y_test, pred_y4, target_names = ['1', '2', '3', '4', '5', '6', '7']))\n",
    "\n",
    "print('[simple decision tree]')\n",
    "print(np.mean(re_tree), np.std(re_tree))\n",
    "print('[ExtraTrees]')\n",
    "print(np.mean(re_extra), np.std(re_random))\n",
    "print('[Random Forest]')\n",
    "print(np.mean(re_random), np.std(re_random))\n",
    "print('[Gradiant boost]')\n",
    "print(np.mean(re_gb), np.std(re_random))\n",
    "print('[xgboost]')\n",
    "print(np.mean(re_xg), np.std(re_xg))\n",
    "print('[SVC]')\n",
    "print(np.mean(re_svc), np.std(re_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# submit based validation\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "train = pd.read_csv('./train.csv').iloc[:, 1:]\n",
    "def merge(df, count):\n",
    "    if count >= 174: ###cover type 1,2를 얼마만큼 중복생성할 것인지 결정.### \n",
    "        return df\n",
    "    add_1 = train.loc[train['Cover_Type'] == 1]\n",
    "    add_2 = train.loc[train['Cover_Type'] == 2]\n",
    "    new_df = pd.concat([add_1, add_2, df], axis=0)\n",
    "    return merge(new_df, count+1)\n",
    "\n",
    "new_train = merge(train, 0)\n",
    "\n",
    "dfX = new_train.iloc[:, :-1]\n",
    "dfy = new_train.iloc[:, [-1]]\n",
    "print(len(dfy))\n",
    "\n",
    "###n_estimators, max_depth 수정### \n",
    "model = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=10, objective='multi:softmax')  \n",
    "#model = XGBClassifier(n_estimators=400, max_depth=7, min_child_weight=4, objective='multi:softmax', nthread = -1)\n",
    "#model = GradientBoostingClassifier(n_estimators=200, max_depth=10) #너무 오래걸림... 약간 손해보더라도 xgboost로 수행\n",
    "result = model.fit(dfX, dfy.values.ravel())\n",
    "\n",
    "#predict\n",
    "test = pd.read_csv('./test.csv').iloc[:, 1:]\n",
    "test_y = result.predict(test)\n",
    "test_y = pd.DataFrame(test_y)\n",
    "\n",
    "#submit file 생성\n",
    "ids = pd.read_csv('./test.csv').iloc[:, :1]\n",
    "submit = pd.concat([ids, test_y], axis=1)\n",
    "submit.columns = ['Id', 'Cover_Type']\n",
    "submit_group = submit.groupby('Cover_Type')\n",
    "print(submit_group.count())\n",
    "submit.to_csv(\"./Submission.csv\", ',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
